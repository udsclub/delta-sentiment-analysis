{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_table('reviews.csv', header=0, error_bad_lines=False, delimiter='|')\n",
    "\n",
    "#split into train and test\n",
    "train, test = train_test_split(df, train_size = 0.7, random_state = 111, stratify=df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove duplicate\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "#reset indices\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TOKENIZATION\n",
    "\n",
    "#regular expressions replacer\n",
    "import re\n",
    "replacement_patterns = [\n",
    "#reductions\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would'),\n",
    "#auotes at the beginig and end\n",
    "(r'^[\"\\'](.+)[\"\\']$','\\g<1>'),\n",
    "#identity nouns - names of films and actors\n",
    "(r'[\"\\'].+?[\"\\']',''),\n",
    "(r'(\\w+)\\s+[A-Z][a-z]+','\\g<1>'),\n",
    "(r'(\\w+)\\s+[A-Z][a-z]+','\\g<1>'),\n",
    "#removing brackets\n",
    "(r'[\\[\\(\\)\\]]',''),\n",
    "#removing dates\n",
    "(r' [\\']?\\d{2,4}s? ',' ')\n",
    "]\n",
    "\n",
    "def replace(text):\n",
    "    for (pattern, repl) in replacement_patterns:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "#tokenizer\n",
    "stopwords_list = [\n",
    "    #pronouns\n",
    "    'i','me','my','mine','myself',\n",
    "    'you','your','yours','yourself',\n",
    "    'he','him','his','himself',\n",
    "    'she','her','hers','herself',\n",
    "    'it','its','itself',\n",
    "    'we','us','our','ours','ourselves',\n",
    "    'you','your','yours','yourselves',\n",
    "    'they','them','their','theirs','themselves',\n",
    "    #acticles\n",
    "    'a','an',\n",
    "    #forms of be\n",
    "    'is','am','are','was','were','will','be',\n",
    "\n",
    "    'to'\n",
    "    ]\n",
    "    \n",
    "               \n",
    "def tokenize_text(text):\n",
    "    replaced = replace(text).lower()\n",
    "    words = word_tokenize(replaced)\n",
    "    #words = [LancasterStemmer().stem(w) for w in words]\n",
    "    words = [SnowballStemmer('english').stem(w) for w in words]\n",
    "    #words = [PorterStemmer().stem(w) for w in words]\n",
    "    #words = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "    #words = [WordNetLemmatizer().lemmatize(w,pos='v') for w in words]\n",
    "    words = [word for word in words if word not in stopwords_list]\n",
    "    return words\n",
    "\n",
    "#vectorization\n",
    "def build_feature_matrices(X):\n",
    "    vectorizer = CountVectorizer(tokenizer=tokenize_text, \n",
    "                                 max_features=4000, ngram_range=(1,2))\n",
    "    X_transform = vectorizer.fit_transform(X).toarray()\n",
    "    features_voc = vectorizer.get_feature_names()\n",
    "    return X_transform, features_voc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NEW FEATURES\n",
    "\n",
    "#review length\n",
    "def review_length(X):\n",
    "    symbol_length = list()\n",
    "    for i in range(len(X)):\n",
    "        length = len(X[i])\n",
    "        symbol_length.append(length)\n",
    "    return symbol_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_transform, features = build_feature_matrices(train['text'])\n",
    "X_train_transform = np.insert(X_train_transform, 0, review_length(train.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XGBOOST\n",
    "\n",
    "#connvert data to xgb format\n",
    "dtrain = xgb.DMatrix(X_train_transform,label=train['label'])\n",
    "\n",
    "#set parameters\n",
    "param = {\n",
    "        'objective':'binary:logistic'\n",
    "        }\n",
    "\n",
    "#train model\n",
    "num_round = 10\n",
    "bst = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TOKENIZATION\n",
    "\n",
    "#regular expressions replacer\n",
    "import re\n",
    "replacement_patterns = [\n",
    "#reductions\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would'),\n",
    "#auotes at the beginig and end\n",
    "(r'^[\"\\'](.+)[\"\\']$','\\g<1>'),\n",
    "#identity nouns - names of films and actors\n",
    "(r'[\"\\'].+?[\"\\']',''),\n",
    "(r'(\\w+)\\s+[A-Z][a-z]+','\\g<1>'),\n",
    "(r'(\\w+)\\s+[A-Z][a-z]+','\\g<1>'),\n",
    "#removing brackets\n",
    "(r'[\\[\\(\\)\\]]',''),\n",
    "#removing dates\n",
    "(r' [\\']?\\d{2,4}s? ',' ')\n",
    "]\n",
    "\n",
    "def replace(text):\n",
    "    for (pattern, repl) in replacement_patterns:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "#tokenizer\n",
    "stopwords_list = [\n",
    "    #pronouns\n",
    "    'i','me','my','mine','myself',\n",
    "    'you','your','yours','yourself',\n",
    "    'he','him','his','himself',\n",
    "    'she','her','hers','herself',\n",
    "    'it','its','itself',\n",
    "    'we','us','our','ours','ourselves',\n",
    "    'you','your','yours','yourselves',\n",
    "    'they','them','their','theirs','themselves',\n",
    "    #acticles\n",
    "    'a','an',\n",
    "    #forms of be\n",
    "    'is','am','are','was','were','will','be',\n",
    "\n",
    "    'to'\n",
    "    ]\n",
    "                \n",
    "def tokenize_text(text):\n",
    "    replaced = replace(text).lower()\n",
    "    words = word_tokenize(replaced)\n",
    "    #words = [LancasterStemmer().stem(w) for w in words]\n",
    "    words = [SnowballStemmer('english').stem(w) for w in words]\n",
    "    #words = [PorterStemmer().stem(w) for w in words]\n",
    "    #words = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "    #words = [WordNetLemmatizer().lemmatize(w,pos='v') for w in words]\n",
    "    words = [word for word in words if word not in stopwords_list]\n",
    "    return words\n",
    "\n",
    "def build_feature_matrices_test(X):\n",
    "    # vectorize using loaded features\n",
    "    vectorizer = CountVectorizer(tokenizer=tokenize_text, vocabulary = features)\n",
    "    #vectorizer = TfidfVectorizer(tokenizer=tokenize_text, vocabulary = features_voc)\n",
    "    X_transform = vectorizer.fit_transform(X).toarray()\n",
    "    return X_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NEW FEATURES\n",
    "\n",
    "#review length\n",
    "def review_length(X):\n",
    "    symbol_length = list()\n",
    "    for i in range(len(X)):\n",
    "        length = len(X[i])\n",
    "        symbol_length.append(length)\n",
    "    return symbol_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_transform = build_feature_matrices_test(test['text'])\n",
    "X_test_transform = np.insert(X_test_transform, 0, review_length(test.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XGBOOST\n",
    "\n",
    "#connvert data to xgb format\n",
    "dtest = xgb.DMatrix(X_test_transform,label=test['label'])\n",
    "\n",
    "predictions_prob = bst.predict(dtest)\n",
    "predictions = [1 if x > 0.55 else 0 for x in predictions_prob]\n",
    "\n",
    "accuracy_score(predictions, test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
